{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"},{"sourceId":177362617,"sourceType":"kernelVersion"},{"sourceId":180008424,"sourceType":"kernelVersion"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import early_stopping,log_evaluation, Dataset, LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import roc_auc_score, r2_score,make_scorer \nfrom sklearn import set_config\nimport warnings\nimport optuna\nfrom sklearn.preprocessing import PolynomialFeatures,OneHotEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom scipy.stats import mode\nfrom sklearn.preprocessing import LabelEncoder\nfrom pprint import pprint\nfrom time import time\nfrom scipy.stats import kurtosis, skew, gmean, mode,trim_mean, mstats\nfrom scipy.stats.mstats import winsorize\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta\n\nwarnings.filterwarnings('ignore')\n\nsns.set_theme(style = 'white', palette = 'viridis')\npal = sns.color_palette('viridis')\n\npd.set_option('display.max_rows', 100)\nset_config(transform_output = 'pandas')\npd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:29.502420Z","iopub.execute_input":"2024-05-28T06:23:29.503205Z","iopub.status.idle":"2024-05-28T06:23:35.511198Z","shell.execute_reply.started":"2024-05-28T06:23:29.503174Z","shell.execute_reply":"2024-05-28T06:23:35.510388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:39.637488Z","iopub.execute_input":"2024-05-28T06:23:39.637854Z","iopub.status.idle":"2024-05-28T06:23:42.874180Z","shell.execute_reply.started":"2024-05-28T06:23:39.637827Z","shell.execute_reply":"2024-05-28T06:23:42.873132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick EDA","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:42.881458Z","iopub.execute_input":"2024-05-28T06:23:42.882078Z","iopub.status.idle":"2024-05-28T06:23:42.938646Z","shell.execute_reply.started":"2024-05-28T06:23:42.882050Z","shell.execute_reply":"2024-05-28T06:23:42.937735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nnumerical_columns = [column for column in train.columns if train[column].dtype in ['int64','float64'] and column not in ['id','FloodProbability']]\n\n#num_subplots = min(len(numerical_columns), 20)  \n#fig, axes = plt.subplots(nrows=10, ncols=2, figsize=(15, 15))\n\n# Plot KDE plots for train and test sets\n#for i in range(num_subplots):\n#    row = i // 2\n#    col = i % 2\n    \n#    sns.kdeplot(data=train, x=numerical_columns[i], ax=axes[row, col], color='blue', label='Train')\n#    sns.kdeplot(data=test, x=numerical_columns[i], ax=axes[row, col], color='orange', label='Test')\n   \n#    axes[row, col].set_title(numerical_columns[i])\n#    axes[row, col].legend()\n\n#plt.tight_layout()\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:44.973739Z","iopub.execute_input":"2024-05-28T06:23:44.974347Z","iopub.status.idle":"2024-05-28T06:23:45.043525Z","shell.execute_reply.started":"2024-05-28T06:23:44.974313Z","shell.execute_reply":"2024-05-28T06:23:45.042487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def  get_gmm_class_features(feat,n):\n    gmm=GaussianMixture(n_components=n,random_state=42)\n    gmm.fit(train[feat].fillna(train[feat].median()).values.reshape(-1,1))\n    train[f'{feat}_class']=gmm.predict(train[feat].fillna(train[feat].median()).values.reshape(-1,1))\n    test[f'{feat}_class']=gmm.predict(test[feat].fillna(test[feat].median()).values.reshape(-1,1))\n\n    \nfor col in numerical_columns:\n    get_gmm_class_features(col,5)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:23:50.485972Z","iopub.execute_input":"2024-05-28T06:23:50.486735Z","iopub.status.idle":"2024-05-28T06:24:58.922662Z","shell.execute_reply.started":"2024-05-28T06:23:50.486701Z","shell.execute_reply":"2024-05-28T06:24:58.921611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"## Preprocessing idea\n# https://www.kaggle.com/code/oscarm524/ps-s4-ep5-eda-modeling-submission#Loading-Libraries\n\nclass Model:\n    def __init__(self, train, test):\n        self.train = train\n        self.test = test\n        self.model_dict = dict()\n        self.test_predict_list = list()\n        self.risk_factors = ['MonsoonIntensity','TopographyDrainage','RiverManagement',\n         'Deforestation','Urbanization','ClimateChange','DamsQuality','Siltation','AgriculturalPractices',\n         'Encroachments','IneffectiveDisasterPreparedness','DrainageSystems','CoastalVulnerability','Landslides',\n         'Watersheds','DeterioratingInfrastructure','PopulationScore',\n         'WetlandLoss','InadequatePlanning','PoliticalFactors']\n        \n    def preprocess(self):\n        for i in np.arange(0.0, 1.01, 0.01):\n            self.train[f'Q_{i}_Risk'] = self.train[self.risk_factors].quantile(i, axis=1)\n            self.test[f'Q_{i}_Risk'] = self.test[self.risk_factors].quantile(i, axis=1)\n        # Calculate risk-related statistics for train data\n        self.train['fsum2'] = self.train[self.risk_factors].product(axis=1)\n        self.train['log_fsum2'] = np.log1p(self.train['fsum2'])\n        self.train['fmin'] = np.linalg.norm(self.train[self.risk_factors], axis=1)\n        self.train['total'] = self.train[self.risk_factors].sum(axis=1)\n        self.train['special1'] = self.train['total'].isin(np.arange(72, 76))\n        self.train['special2'] = self.train['total'].isin(np.arange(87, 94))\n        self.train['Avg_Risk'] = self.train[self.risk_factors].mean(axis=1)\n        self.train['Med_Risk'] = self.train[self.risk_factors].median(axis=1)\n#         self.train['Q5_Risk'] = self.train[self.risk_factors].quantile(0.05, axis=1)\n#         self.train['Q10_Risk'] = self.train[self.risk_factors].quantile(0.10, axis=1)\n#         self.train['Q15_Risk'] = self.train[self.risk_factors].quantile(0.15, axis=1)\n#         self.train['Q20_Risk'] = self.train[self.risk_factors].quantile(0.20, axis=1)\n#         self.train['Q25_Risk'] = self.train[self.risk_factors].quantile(0.25, axis=1)\n#         self.train['Q30_Risk'] = self.train[self.risk_factors].quantile(0.30, axis=1)\n#         self.train['Q35_Risk'] = self.train[self.risk_factors].quantile(0.35, axis=1)\n#         self.train['Q40_Risk'] = self.train[self.risk_factors].quantile(0.40, axis=1)\n#         self.train['Q45_Risk'] = self.train[self.risk_factors].quantile(0.45, axis=1)\n#         self.train['Q50_Risk'] = self.train[self.risk_factors].quantile(0.50, axis=1)\n#         self.train['Q55_Risk'] = self.train[self.risk_factors].quantile(0.55, axis=1)\n#         self.train['Q60_Risk'] = self.train[self.risk_factors].quantile(0.60, axis=1)\n#         self.train['Q65_Risk'] = self.train[self.risk_factors].quantile(0.65, axis=1)\n#         self.train['Q70_Risk'] = self.train[self.risk_factors].quantile(0.70, axis=1)\n#         self.train['Q72_Risk'] = self.train[self.risk_factors].quantile(0.72, axis=1)\n#         self.train['Q73_Risk'] = self.train[self.risk_factors].quantile(0.73, axis=1)\n#         self.train['Q74_Risk'] = self.train[self.risk_factors].quantile(0.74, axis=1)\n#         self.train['Q75_Risk'] = self.train[self.risk_factors].quantile(0.75, axis=1)\n#         self.train['Q76_Risk'] = self.train[self.risk_factors].quantile(0.76, axis=1)\n#         self.train['Q80_Risk'] = self.train[self.risk_factors].quantile(0.80, axis=1)\n#         self.train['Q85_Risk'] = self.train[self.risk_factors].quantile(0.85, axis=1)\n#         self.train['Q90_Risk'] = self.train[self.risk_factors].quantile(0.90, axis=1)\n#         self.train['Q95_Risk'] = self.train[self.risk_factors].quantile(0.95, axis=1)\n#         self.train['Q100_Risk'] = self.train[self.risk_factors].quantile(1.0, axis=1)\n        self.train['Std_Risk'] = self.train[self.risk_factors].std(axis=1)\n        self.train['Min_Risk'] = self.train[self.risk_factors].min(axis=1)\n        self.train['Max_Risk'] = self.train[self.risk_factors].max(axis=1)\n        self.train['fsqt'] = self.train[self.risk_factors].apply(np.sqrt).sum(axis=1)\n        self.train['range'] = self.train['Max_Risk'] - self.train['Min_Risk']\n        self.train['Variance'] = self.train[self.risk_factors].var(axis=1)\n        self.train['Kurtosis'] = kurtosis(self.train[self.risk_factors], axis=1)\n        self.train['Skew'] = skew(self.train[self.risk_factors], axis=1)\n        self.train['gmean'] = gmean(self.train[self.risk_factors], axis=1)\n        self.train['ptp'] = self.train[self.risk_factors].values.ptp(axis=1)\n        self.train['mode'] = mode(self.train[self.risk_factors], axis=1)[0]\n        self.train['IQR'] = self.train[self.risk_factors].quantile(0.75, axis=1) - self.train[self.risk_factors].quantile(0.25, axis=1)\n        self.train['CV'] = self.train[self.risk_factors].std(axis=1) / self.train[self.risk_factors].mean(axis=1)\n        self.train['std_range'] = self.train['Std_Risk'] / self.train['range']\n        self.train['mean_range'] = self.train['Avg_Risk'] / self.train['range']\n        self.train['std_mean'] = self.train['Std_Risk'] / self.train['Avg_Risk']\n        self.train['std_median'] = self.train['Std_Risk'] / self.train['Med_Risk']\n        self.train['mean_median'] = self.train['Avg_Risk'] / self.train['Med_Risk']\n        self.train['std_fsum2'] = self.train['Std_Risk'] / self.train['fsum2']\n        self.train['mean_fsum2'] = self.train['Avg_Risk'] / self.train['fsum2']\n        self.train['std_fsqt'] = self.train['Std_Risk'] / self.train['fsqt']\n        self.train['mean_fsqt'] = self.train['Avg_Risk'] / self.train['fsqt']\n        self.train['std_log_fsum'] = self.train['Std_Risk'] / self.train['log_fsum2']\n        self.train['fsum_sq'] = self.train['fsum2'] ** 2\n        self.train['fsum_cu'] = self.train['fsum2'] ** 3\n        self.train['euclidean_distance'] = np.sqrt((self.train[self.risk_factors]**2).sum(axis=1))\n        self.train['harmonic'] = len(self.risk_factors) / self.train[self.risk_factors].apply(lambda x: (1/x).mean(), axis=1)\n        self.train['zscore'] = self.train[self.risk_factors].apply(lambda x: (x - x.mean()) / x.std(), axis=1).mean(axis=1)\n        self.train['cv'] = self.train[self.risk_factors].std(axis=1) / self.train[self.risk_factors].mean(axis=1)\n        self.train['Skewness_75'] = (self.train[self.risk_factors].quantile(0.75, axis=1) - self.train[self.risk_factors].mean(axis=1)) / self.train[self.risk_factors].std(axis=1)\n        self.train['Skewness_25'] = (self.train[self.risk_factors].quantile(0.25, axis=1) - self.train[self.risk_factors].mean(axis=1)) / self.train[self.risk_factors].std(axis=1)\n        self.train['2ndMoment'] = self.train[self.risk_factors].apply(lambda x: (x**2).mean(), axis=1)\n        self.train['3rdMoment'] = self.train[self.risk_factors].apply(lambda x: (x**3).mean(), axis=1)\n        self.train['entropy'] = self.train[self.risk_factors].apply(lambda x: -1*(x*np.log(x)).sum(), axis=1)\n        \n\n        self.train['ClimateImpact'] = self.train['MonsoonIntensity'] + self.train['ClimateChange']\n        self.train['AnthropogenicPressure'] = self.train['Deforestation'] + self.train['Urbanization'] + self.train['AgriculturalPractices'] + self.train['Encroachments']\n        self.train['InfrastructureQuality'] = self.train['DamsQuality'] + self.train['DrainageSystems'] + self.train['DeterioratingInfrastructure']\n        self.train['CoastalVulnerabilityTotal'] = self.train['CoastalVulnerability'] + self.train['Landslides']\n        self.train['PreventiveMeasuresEfficiency'] = self.train['RiverManagement'] + self.train['IneffectiveDisasterPreparedness'] + self.train['InadequatePlanning']\n        self.train['EcosystemImpact'] = self.train['WetlandLoss'] + self.train['Watersheds']\n        self.train['SocioPoliticalContext'] = self.train['PopulationScore'] * self.train['PoliticalFactors']\n        self.train['ClimateImpact_Urbanization'] = self.train['MonsoonIntensity'] * self.train['Urbanization']\n        self.train['DamsQuality_Siltation'] = self.train['DamsQuality'] * self.train['Siltation']\n        self.train['Encroachments_Landslides'] = self.train['Encroachments'] * self.train['Landslides']\n        self.train['Deforestation_Urbanization'] = self.train['Deforestation'] / self.train['Urbanization']\n        self.train['DrainageSystems_DeterioratingInfrastructure'] = self.train['DrainageSystems'] / self.train['DeterioratingInfrastructure']\n        self.train['CoastalVulnerability_Landslides'] = self.train['CoastalVulnerability'] / self.train['Landslides']\n\n        # Calculate risk-related statistics for test data\n        self.test['fsum2'] = self.test[self.risk_factors].product(axis=1)\n        self.test['log_fsum2'] = np.log1p(self.test['fsum2'])\n        self.test['fmin'] = np.linalg.norm(self.test[self.risk_factors], axis=1)\n        self.test['total'] = self.test[self.risk_factors].sum(axis=1)\n        self.test['special1'] = self.test['total'].isin(np.arange(72, 76))\n        self.test['special2'] = self.test['total'].isin(np.arange(87, 94))\n        self.test['Avg_Risk'] = self.test[self.risk_factors].mean(axis=1)\n        self.test['Med_Risk'] = self.test[self.risk_factors].median(axis=1)\n#         self.test['Q5_Risk'] = self.test[self.risk_factors].quantile(0.05, axis=1)\n#         self.test['Q10_Risk'] = self.test[self.risk_factors].quantile(0.10, axis=1)\n#         self.test['Q15_Risk'] = self.test[self.risk_factors].quantile(0.15, axis=1)\n#         self.test['Q20_Risk'] = self.test[self.risk_factors].quantile(0.20, axis=1)\n#         self.test['Q25_Risk'] = self.test[self.risk_factors].quantile(0.25, axis=1)\n#         self.test['Q30_Risk'] = self.test[self.risk_factors].quantile(0.30, axis=1)\n#         self.test['Q35_Risk'] = self.test[self.risk_factors].quantile(0.35, axis=1)\n#         self.test['Q40_Risk'] = self.test[self.risk_factors].quantile(0.40, axis=1)\n#         self.test['Q45_Risk'] = self.test[self.risk_factors].quantile(0.45, axis=1)\n#         self.test['Q50_Risk'] = self.test[self.risk_factors].quantile(0.50, axis=1)\n#         self.test['Q55_Risk'] = self.test[self.risk_factors].quantile(0.55, axis=1)\n#         self.test['Q60_Risk'] = self.test[self.risk_factors].quantile(0.60, axis=1)\n#         self.test['Q65_Risk'] = self.test[self.risk_factors].quantile(0.65, axis=1)\n#         self.test['Q70_Risk'] = self.test[self.risk_factors].quantile(0.70, axis=1)\n#         self.test['Q72_Risk'] = self.test[self.risk_factors].quantile(0.72, axis=1)\n#         self.test['Q73_Risk'] = self.test[self.risk_factors].quantile(0.73, axis=1)\n#         self.test['Q74_Risk'] = self.test[self.risk_factors].quantile(0.74, axis=1)\n#         self.test['Q75_Risk'] = self.test[self.risk_factors].quantile(0.75, axis=1)\n#         self.test['Q76_Risk'] = self.test[self.risk_factors].quantile(0.76, axis=1)\n#         self.test['Q80_Risk'] = self.test[self.risk_factors].quantile(0.80, axis=1)\n#         self.test['Q85_Risk'] = self.test[self.risk_factors].quantile(0.85, axis=1)\n#         self.test['Q90_Risk'] = self.test[self.risk_factors].quantile(0.90, axis=1)\n#         self.test['Q95_Risk'] = self.test[self.risk_factors].quantile(0.95, axis=1)\n#         self.test['Q100_Risk'] = self.test[self.risk_factors].quantile(1.0, axis=1)\n        self.test['Std_Risk'] = self.test[self.risk_factors].std(axis=1)\n        self.test['Min_Risk'] = self.test[self.risk_factors].min(axis=1)\n        self.test['Max_Risk'] = self.test[self.risk_factors].max(axis=1)\n        self.test['fsqt'] = self.test[self.risk_factors].apply(np.sqrt).sum(axis=1)\n        self.test['range'] = self.test['Max_Risk'] - self.test['Min_Risk']\n        self.test['Variance'] = self.test[self.risk_factors].var(axis=1)\n        self.test['Kurtosis'] = kurtosis(self.test[self.risk_factors], axis=1)\n        self.test['Skew'] = skew(self.test[self.risk_factors], axis=1)\n        self.test['gmean'] = gmean(self.test[self.risk_factors], axis=1)\n        self.test['ptp'] = self.test[self.risk_factors].values.ptp(axis=1)\n        self.test['mode'] = mode(self.test[self.risk_factors], axis=1)[0]\n        self.test['IQR'] = self.test[self.risk_factors].quantile(0.75, axis=1) - self.test[self.risk_factors].quantile(0.25, axis=1)\n        self.test['CV'] = self.test[self.risk_factors].std(axis=1) / self.test[self.risk_factors].mean(axis=1)\n        self.test['std_range'] = self.test['Std_Risk'] / self.test['range']\n        self.test['mean_range'] = self.test['Avg_Risk'] / self.test['range']\n        self.test['std_mean'] = self.test['Std_Risk'] / self.test['Avg_Risk']\n        self.test['std_median'] = self.test['Std_Risk'] / self.test['Med_Risk']\n        self.test['mean_median'] = self.test['Avg_Risk'] / self.test['Med_Risk']\n        self.test['std_fsum2'] = self.test['Std_Risk'] / self.test['fsum2']\n        self.test['mean_fsum2'] = self.test['Avg_Risk'] / self.test['fsum2']\n        self.test['std_fsqt'] = self.test['Std_Risk'] / self.test['fsqt']\n        self.test['mean_fsqt'] = self.test['Avg_Risk'] / self.test['fsqt']\n        self.test['std_log_fsum'] = self.test['Std_Risk'] / self.test['log_fsum2']\n        self.test['fsum_sq'] = self.test['fsum2'] ** 2\n        self.test['fsum_cu'] = self.test['fsum2'] ** 3\n        self.test['euclidean_distance'] = np.sqrt((self.test[self.risk_factors]**2).sum(axis=1))\n        self.test['harmonic'] = len(self.risk_factors) / self.test[self.risk_factors].apply(lambda x: (1/x).mean(), axis=1)\n        self.test['zscore'] = self.test[self.risk_factors].apply(lambda x: (x - x.mean()) / x.std(), axis=1).mean(axis=1)\n        self.test['cv'] = self.test[self.risk_factors].std(axis=1) / self.test[self.risk_factors].mean(axis=1)\n        self.test['Skewness_75'] = (self.test[self.risk_factors].quantile(0.75, axis=1) - self.test[self.risk_factors].mean(axis=1)) / self.test[self.risk_factors].std(axis=1)\n        self.test['Skewness_25'] = (self.test[self.risk_factors].quantile(0.25, axis=1) - self.test[self.risk_factors].mean(axis=1)) / self.test[self.risk_factors].std(axis=1)\n        self.test['2ndMoment'] = self.test[self.risk_factors].apply(lambda x: (x**2).mean(), axis=1)\n        self.test['3rdMoment'] = self.test[self.risk_factors].apply(lambda x: (x**3).mean(), axis=1)\n        self.test['entropy'] = self.test[self.risk_factors].apply(lambda x: -1*(x*np.log(x)).sum(), axis=1)\n      \n        self.test['ClimateImpact'] = self.test['MonsoonIntensity'] + self.test['ClimateChange']\n        self.test['AnthropogenicPressure'] = self.test['Deforestation'] + self.test['Urbanization'] + self.test['AgriculturalPractices'] + self.test['Encroachments']\n        self.test['InfrastructureQuality'] = self.test['DamsQuality'] + self.test['DrainageSystems'] + self.test['DeterioratingInfrastructure']\n        self.test['CoastalVulnerabilityTotal'] = self.test['CoastalVulnerability'] + self.test['Landslides']\n        self.test['PreventiveMeasuresEfficiency'] = self.test['RiverManagement'] + self.test['IneffectiveDisasterPreparedness'] + self.test['InadequatePlanning']\n        self.test['EcosystemImpact'] = self.test['WetlandLoss'] + self.test['Watersheds']\n        self.test['SocioPoliticalContext'] = self.test['PopulationScore'] * self.test['PoliticalFactors']\n        self.test['ClimateImpact_Urbanization'] = self.test['MonsoonIntensity'] * self.test['Urbanization']\n        self.test['DamsQuality_Siltation'] = self.test['DamsQuality'] * self.test['Siltation']\n        self.test['Encroachments_Landslides'] = self.test['Encroachments'] * self.test['Landslides']\n        self.test['Deforestation_Urbanization'] = self.test['Deforestation'] / self.test['Urbanization']\n        self.test['DrainageSystems_DeterioratingInfrastructure'] = self.test['DrainageSystems'] / self.test['DeterioratingInfrastructure']\n        self.test['CoastalVulnerability_Landslides'] = self.test['CoastalVulnerability'] / self.test['Landslides']\n\n        ## Unique Values\n        unique_vals = []\n        for df in [self.train, self.test]:\n            for col in self.risk_factors:\n                unique_vals += list(df[col].unique())\n\n        unique_vals = list(set(unique_vals))\n        for df in [self.train, self.test]:\n            for v in unique_vals:\n                df['cnt_{}'.format(v)] = (df[self.risk_factors] == v).sum(axis=1)\n                \n        self.add_transform_features(self.risk_factors)\n                \n    \n    def add_transform_features(self, initial_features):\n        log_features = [f\"log_{col}\" for col in initial_features]\n        log2_features = [f\"log2_{col}\" for col in initial_features]\n        exp_features = [f\"exp_{col}\" for col in initial_features]\n        exp2_features = [f\"exp2_{col}\" for col in initial_features]\n        exp3_features = [f\"exp3_{col}\" for col in initial_features]\n        exp4_features = [f\"exp4_{col}\" for col in initial_features]\n\n        for df in [self.train, self.test]:\n            for col in initial_features:\n                df[f\"log_{col}\"] = np.log1p(df[col] + 1e-4)\n            df['log_sum'] = df[log_features].sum(axis=1)\n\n            for col in initial_features:\n                df[f\"log2_{col}\"] = np.log10(df[col] + 1e-4)\n            df['log2_sum'] = df[log2_features].sum(axis=1)\n\n            for col in initial_features:\n                df[f\"exp_{col}\"] = 1.2**(df[col])\n            df['exp_sum'] = df[exp_features].sum(axis=1)\n\n            for col in initial_features:\n                df[f\"exp2_{col}\"] = 2.5**(df[col])\n            df['exp2_sum'] = df[exp2_features].mean(axis=1)\n\n            for col in initial_features:\n                df[f\"exp3_{col}\"] = 4**(df[col])\n            df['exp3_sum'] = df[exp3_features].mean(axis=1)\n\n            for col in initial_features:\n                df[f\"exp4_{col}\"] = 2**(df[col])\n            df['exp4_sum'] = df[exp4_features].sum(axis=1)\n\n            df['sum_log'] = np.log1p(df['total'])\n        \n    def fit(self,params,name):\n        self.preprocess()\n        target_col = ['FloodProbability']\n        drop_col = ['id']\n        \n        train_cols = [col for col in self.train.columns.to_list() if col not in target_col + drop_col]\n        scores = list()\n        \n        \n        for i in range(1):\n            mskf = RepeatedKFold(n_splits=5, n_repeats=1,random_state=42)\n            oof_valid_preds = np.zeros(self.train[train_cols].shape[0])\n                \n            for fold, (train_idx, valid_idx) in enumerate(mskf.split(self.train[train_cols], self.train[target_col])):\n                X_train, y_train = self.train[train_cols].iloc[train_idx], self.train[target_col].iloc[train_idx]\n                X_valid, y_valid = self.train[train_cols].iloc[valid_idx], self.train[target_col].iloc[valid_idx]\n                \n                if name == 'lgbm':\n                    algo = LGBMRegressor(random_state=i+fold,**params)\n                    algo.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n                elif name== 'catboost':\n                    algo = CatBoostRegressor(random_state=i+fold,**params)\n                else:\n                    algo = XGBRegressor(random_state=i+fold, missing=float('inf'),**params)\n                    algo.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=False)\n                    \n                    \n                #algo.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=False)\n                    \n                valid_preds = algo.predict(X_valid)\n                oof_valid_preds[valid_idx] = valid_preds\n                test_predict = algo.predict(self.test[train_cols])\n                self.test_predict_list.append(test_predict)\n                score = r2_score(y_valid, valid_preds)\n                print(f\"The r2 score for fold {fold+1} is {score}\")\n                self.model_dict[f'fold_{fold}'] = algo\n                    \n            oof_score = r2_score(self.train[target_col], oof_valid_preds)\n            print(f\"The OOF r2 score for iteration {i+1} is {oof_score}\")\n            scores.append(oof_score)\n        return scores,self.test_predict_list\n    \n    def report_perf(self,optimizer, X, y, title, callbacks=None):\n        \"\"\"\n        A wrapper for measuring time and performances of different optmizers\n\n        optimizer = a sklearn or a skopt optimizer\n        X = the training set \n        y = our target\n        title = a string label for the experiment\n        \"\"\"\n        start = time()\n        if callbacks:\n            optimizer.fit(X, y, callback=callbacks)\n        else:\n            optimizer.fit(X, y)\n        best_score = optimizer.best_score_\n        best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n        best_params = optimizer.best_params_\n        print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n               +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                      len(optimizer.cv_results_['params']),\n                                      best_score,\n                                      best_score_std))    \n        print('Best parameters:')\n        pprint(best_params)\n        print()\n        return best_params\n    \n    \n    def find_params(self):\n        self.preprocess()\n        avg_r2 = make_scorer(r2_score, greater_is_better=True, needs_proba=False)\n        search_spaces = {\n            'num_leaves': Integer(2, 210, prior='uniform'),\n            'learning_rate': Real(0.001, 0.1, prior='log-uniform'),\n            'n_estimators': Integer(50, 1000, prior='uniform'),\n            'subsample_for_bin': Integer(10000, 500000, prior='uniform'),\n            'min_child_samples': Integer(1, 100, prior='uniform'),\n            'reg_alpha': Real(1e-10, 1e-5, prior='log-uniform'),\n            'reg_lambda': Real(1e-10, 1e-5, prior='log-uniform'),\n            'colsample_bytree': Real(0.1, 0.9, prior='uniform'),\n            'subsample': Real(0.5, 1.0, prior='uniform'),\n            'max_depth': Integer(2, 20, prior='uniform')\n        }\n        skf = RepeatedKFold(n_splits=10, n_repeats=1,random_state=42)\n\n        opt = BayesSearchCV(LGBMRegressor(verbosity=-1,device='gpu'),\n                    search_spaces,\n                    scoring=avg_r2,\n                    cv=skf,\n                    n_iter=10000,\n                    n_jobs=1,  \n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=22)\n        target_col = ['FloodProbability']\n        drop_col = ['id']\n        train_cols = [col for col in self.train.columns.to_list() if col not in target_col + drop_col]\n        X = self.train[train_cols]\n        y = self.train[target_col]\n        best_params = self.report_perf(opt, X, y,'LightBoost', \n                          callbacks=[DeltaXStopper(0.001), \n                                     DeadlineStopper(60*60*11)])\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:25:43.936864Z","iopub.execute_input":"2024-05-28T06:25:43.937251Z","iopub.status.idle":"2024-05-28T06:25:44.042979Z","shell.execute_reply.started":"2024-05-28T06:25:43.937201Z","shell.execute_reply":"2024-05-28T06:25:44.042105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_params1 = {\n    'num_leaves': 183,\n    'learning_rate': 0.01183688880802108,\n    'n_estimators': 577,\n    'subsample_for_bin': 165697,\n    'min_child_samples': 114,\n    'reg_alpha': 2.075080888948164e-06,\n    'reg_lambda': 3.838938366471552e-07,\n    'colsample_bytree': 0.9634044234652241,\n    'subsample': 0.9592138618622019,\n    'max_depth': 9,\n    #'device':'gpu',\n        'verbosity':-1,\n}\n\nmodel = Model(train,test)\nlgbm_scores1,lgbm_preds1 = model.fit(lgbm_params1,'lgbm')\nprint(f'The average r2 score is {np.mean(lgbm_scores1)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T06:25:45.230715Z","iopub.execute_input":"2024-05-28T06:25:45.231429Z","iopub.status.idle":"2024-05-28T07:03:04.349813Z","shell.execute_reply.started":"2024-05-28T06:25:45.231397Z","shell.execute_reply":"2024-05-28T07:03:04.348872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_params = {\n        'num_leaves': 210, \n        'learning_rate': 0.00895710669100346, \n        'n_estimators': 727, \n        'subsample_for_bin': 21372, \n        'min_child_samples': 100, \n        'reg_alpha': 4.5427528680311086e-07, \n        'reg_lambda': 6.3824100237054236e-09, \n        'colsample_bytree': 0.565257393643049, \n        'subsample': 0.5774186207711538, \n        'max_depth': 11,\n        #'device':'gpu',\n        'verbosity':-1,\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T06:05:11.861501Z","iopub.execute_input":"2024-05-18T06:05:11.861825Z","iopub.status.idle":"2024-05-18T06:05:11.868506Z","shell.execute_reply.started":"2024-05-18T06:05:11.861797Z","shell.execute_reply":"2024-05-18T06:05:11.867476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(train,test)\nlgbm_scores,lgbm_preds = model.fit(lgbm_params,'lgbm')\nprint(f'The average r2 score is {np.mean(lgbm_scores)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"gluon_preds = pd.read_csv('/kaggle/input/autogluon-starter/submission.csv').FloodProbability\nvoting_preds = pd.read_csv('/kaggle/input/flood-prediction-lightboost-oof-preds/submission.csv').FloodProbability\nlgbm_prediction = np.mean(lgbm_preds,axis=0)\n\nfinal_preds = 0.05*np.mean(lgbm_preds1,axis=0)+lgbm_prediction*0.35 + 0.35* gluon_preds + 0.25 * voting_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv('/kaggle/input/playground-series-s4e5/sample_submission.csv')\nsubmit.FloodProbability = final_preds\nsubmit.to_csv('submission.csv',index=False)\nsubmit","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}